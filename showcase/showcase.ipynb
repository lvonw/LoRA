{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fine-Tuning using LoRA**\n",
    "\n",
    "This notebook aims to provide you with all the necessary information to be able to effectively use the state of the art fine-tuning frameworks for your specific use case and how to integrate them into your environment.\n",
    "To this end this guide will include the steps for Huggingfaces `PEFT` implementation, `unsloth` and `torchtune`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **When to use fine-tuning**\n",
    "\n",
    "Fine-tuning can be an effective way to customize an LLM to your specific needs, however it is not a silver bullet and for some use cases other techniques, such as RAG may be better suited. \n",
    "In particular fine-tuning is best used to influence the \"behaviour\" of an LLM - the speech patterns, phrasing and the like. When it comes to accurately representing facts from your internal knowledge-base, it becomes very challenging and requires a lot of data in order to do this reliably. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **General Dependencies**\n",
    "\n",
    "There are a few general dependancies for this guide that should be met beforehand. Firstly a `Python` environment running at least version `3.9` as well as a `PyTorch` installation of version `2.6.0` or later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up a remote environment**\n",
    "\n",
    "This section will go into detail how you can setup your own remote working environment on a machine with a business grade GPU. If you have a powerful enough GPU you can also run everything on your own machine, so feel free to skip this if that is an option for you.\n",
    "\n",
    "We will be focusing on a single GPU Machine. There are some extra considerations you'll have to make in a multi-gpu or even node-cluster, but for the purposes of this demonstration that is overkill. Most of that is handled by the frameworks themselves anyway.\n",
    "\n",
    "There are many hosting services out there. From my personal experience and research I can recommend paperspace. They provide very reliable machines and handle the whole networking part fairly seemlessly for you. The pricing is also very reasonable in a pey-per-hour way. \n",
    "\n",
    "### **Basics**\n",
    "\n",
    "For our purposes an NVidia a6000 GPU is sufficient. It's pretty powerful and modern enough to support the newest CUDA versions, which some of the frameworks used later require. \n",
    "\n",
    "You can either use the ML-In-A-Box Environment or start from a fresh Linux distro. The first option is convenient because it already has CUDA installed. But it can be a gamble if it's the right version. If it isn't you'll need to install the new one yourself. Like you would have to for the fresh distro. \n",
    "\n",
    "Once youve set up the machine connect to it using `ssh paperspace@[HOST-IP]`\n",
    "\n",
    "From there check the CUDA version (if any) via \n",
    "```\n",
    "nvcc --version\n",
    "```\n",
    "You'll be looking for any version higher than 12.0.\n",
    "\n",
    "The following command can be rather slow and might not be necessary if your CUDA version does not need to be changed.\n",
    "\n",
    "```\n",
    "sudo apt-get update\n",
    "```\n",
    "\n",
    "To install the right CUDA version manually check the [NVidia documentation](https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_network) \n",
    "\n",
    "### **Installing Conda** \n",
    "\n",
    "Conda is very convenient for easy environment management. Even within a conda env you can use pip to download only for that env, so there's really no downside. \n",
    "\n",
    "To install it run the following 4 commands sequentially\n",
    "```\n",
    "mkdir -p ~/miniconda3\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n",
    "bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n",
    "rm ~/miniconda3/miniconda.sh\n",
    "```\n",
    "\n",
    "You will then have to configure the PATH environment variable to ensure the \n",
    "conda command is found in the terminal. To do this run the following command\n",
    "in the Terminal which will open the NANO editor in which you can configure the\n",
    "environment.\n",
    "\n",
    "```\n",
    "nano ~/.bashrc\n",
    "```\n",
    "\n",
    "Once opened add the following to the very end of the file, then press `ctrl+x` \n",
    "to close NANO, follow with `y` to save the changes and `enter` to apply the\n",
    "changes to the current file.\n",
    "\n",
    "```\n",
    "add EXPORT PATH=\"/home/paperspace/miniconda3/bin:$PATH\"\n",
    "```\n",
    " \n",
    "For a sanity check you can print the PATH variable using `echo $PATH`. After \n",
    "you have done this you will have to reload the path with\n",
    "\n",
    "```\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "Now in order to be able to use conda effectively you will have to run the \n",
    "following 2 commands after which you should see `(base)` at the start of your\n",
    "terminal prompt.\n",
    "\n",
    "```\n",
    "conda init\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "### **Installing all dependancies with conda**\n",
    "\n",
    "First create your conda env using the folling command.\n",
    "\n",
    "```\n",
    "conda create --name lora python=3.11\n",
    "```\n",
    "\n",
    "After you created it you will have to activate it with \n",
    "\n",
    "```\n",
    "conda activate lora \n",
    "```\n",
    "\n",
    "Now the `(base)` in the command line should show `(lora)` or whatever name you decided to give your environment.\n",
    "\n",
    "From there you can install all the dependencies with either `pip install` or `conda install`.\n",
    "\n",
    "### **Cloning the Git Repo**\n",
    "\n",
    "To clone your git repository use this command and verify using your GitHub\n",
    "username and access token for the project\n",
    "\n",
    "```\n",
    "git clone [REPO LINK]\n",
    "```\n",
    "\n",
    "If you dont want to enter the credentials everytime you do a git action you can\n",
    "cache them for a certain amount time. Simply run this command once before\n",
    "a git action. You can change the amount of time for however long you want. \n",
    "\n",
    "```\n",
    "git config --global credential.helper 'cache --timeout=3600'\n",
    "```\n",
    "\n",
    "Note however that this lasts only for the current session. So for the first action of any session you will have to put in your credentials once.\n",
    "\n",
    "If you have all dependencies installed you should now be good to go to run your scripts as you normally would.\n",
    "\n",
    "### **Mounting a Network Drive**\n",
    "\n",
    "On-Device storage can be quite expensive, so if you need more using a network drive is often a better solution. However in order to be able to use this network drive you will have to mount it. This means that the external drive will be treated like a normal directory in your file-system, making all of the networking completely transparent!\n",
    "\n",
    "To mount the network drive you may first need to create a directory you want to mount the drive to. I recommend first navigating to your repositories root directory, for more convenient access. Then after create the new folder. \n",
    "`mkdir ./data`\n",
    "\n",
    "Afterwards you edit the permission for mounting outside of the root directory\n",
    "using \n",
    "\n",
    "```\n",
    "sudo chown paperspace:paperspace /home/paperspace/PATH/TO/MOUNT\n",
    "```\n",
    "\n",
    "Then you mount the drive by editing the fstab NANO\n",
    "\n",
    "```\n",
    "sudo nano /etc/fstab\n",
    "```\n",
    "\n",
    "Simply add this command at the bottom\n",
    "\n",
    "```\n",
    "//your-shared-drive-ip-address/your-shared-drive-name /home/paperspace/PATH/TO/MOUNT   cifs  username=your-username,password=your-password,uid=1000,gid=1000,rw,user  0  0\n",
    "```\n",
    "\n",
    "Lastly now you can mount the directory you want with the call. Simply add the\n",
    "same file you wrote into /etc/fstab \n",
    "\n",
    "```\n",
    "mount /home/paperspace/PATH/TO/MOUNT\n",
    "```\n",
    "\n",
    "You can verify whether this worked with  `df -h`\n",
    "\n",
    "### **Accessing Remote Data**\n",
    "\n",
    "To download data from your machine run \n",
    "\n",
    "```\n",
    "scp paperspace@host-ip:complete-host-path path-to-copy-to\n",
    "```\n",
    "\n",
    "Conversely to upload Data\n",
    "\n",
    "```\n",
    "scp path-to-upload paperspace@host-ip:complete-host-path \n",
    "```\n",
    "\n",
    "To see the full path of a folder on your machine you can get the full path via \n",
    "`pwd`\n",
    "\n",
    "If you uploaded an archive you extract the data via\n",
    "\n",
    "```\n",
    "tar -xzvf filename.tar.gz\n",
    "```\n",
    "\n",
    "### **Monitoring the Systems Performance**\n",
    "\n",
    "You can do `ls -l` to check when the files in the current directory where last \n",
    "changed \n",
    "\n",
    "You can see a file updating real time while looking at the bottom lines using\n",
    "`tail -f -n 20 ./`\n",
    "\n",
    "Check what processes are running with `htop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Selecting a Base Model**\n",
    "\n",
    "Selecting a base model, that is the model that we will train the adapter for, should be the first decision one should make, as it may have implications on which frameworks or data formats are compatible.  While the options for competitive models are ever expanding, making it hard to recommend one specific one for all cases, there are a few key considerations as to which model you should pick to keep in mind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Foundation vs. Instruction Model**\n",
    "\n",
    "The first and perhaps easiest choice to make is whether to go for a instructional model or not. For most normal use cases it would be enough to just go with the instructional one and move on, however let's have a look at what this actually means. \n",
    "Instructional models are in and of themselves already fine-tuned to expect a specific message format, which influences their behaviour. As the name would suggest this format is structured around certain instructions, each of which are intended to handle different aspects of the models behaviour. In the vast majority of cases there are three different kinds of instructions:\n",
    "\n",
    "1. `System`: This instruction is directed at the model itself and influences the general behaviour of a model, rather than a specific question. \n",
    "2. `User`: This is the input the model should generate a response to. \n",
    "3. `Assistant`: This is the models response. \n",
    "\n",
    "As you may have noticed these are very similar and in fact essentially the same as the different types of prompts that are relevant when interfacing with popular models such as the GPTs. This also gives you a good pointer to when to use these models: Whenever you're aiming to have some sort of user interaction with the model.\n",
    "\n",
    "Foundational models do not have this condition for their inputs. They simply take in any text and generate likely continuations. This makes them essentially like a blank canvas, for you to do whatever with. Which may be useful if youre looking to generate text without any user influence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Open vs. Closed Source**\n",
    "\n",
    "As with most other software applications, this is another important consideration to make. Closed source models are often the more powerful alternatives. However the key advantages of open source models are first that you have complete control over your internal data - theres no obligation to send any data to an external service provider. Second, due to being able to run the models on the hardware of your choice you have much greater control over the environment the model is run on, which in turn affects the financial aspects of your application. \n",
    "\n",
    "Noteworthy however is the respective open-source license as many apply additional restrictions and conditions as to the intended use case. For research and educational purposes, most of these are fairly unlimited.\n",
    "\n",
    "For the purposes of this demonstration we will go forth with open-source models as that allows to show the implementation and integration of the complete fine-tuning process into your personal environment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Sizes**\n",
    "\n",
    "Model sizes are measured in the amount of parameters they are composed of. This number can range from one billion into the trillions. Generally speaking the more parameters - the more powerful the model is. However with increased parameter count comes an increase in required ressources as these models live on your GPU's VRAM during runtime. So your hardware hard limits the model choice in terms of parameter count. In general if youre running this on your local PC the choice is usually limited from 1B to 8B parameters. Which can give you decent performance already, but its definitely on the low end of the spectrum. For higher tier models you will need a business grade GPU as we've already explored in the previous section (...or even a cluster).\n",
    "\n",
    "One important note is that training requires a lot more VRAM than just running a model for inference, so fine tuning a model will quickly become a very intensive task even for high end consumer GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Options**\n",
    "\n",
    "As mentioned before there are many options for viable open source models, and the list is ever expanding. This guide will focus on the Llama 3 model family as they provide a great range of different model sizes, all of which show great performance for their respective parameter count, which makes quick prototyping very easy. For reference there is also an included test using a MistralAI model later on.\n",
    "The shown implementations are however fairly model agnostic so it should be straight forward to switch one out for another in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Downloading the Models**\n",
    "\n",
    "The de-facto standard for LLMs when it comes to downloading and using them is `Huggingface` - a platform that provides access to essentially all available open-source LLMs, as well as a lots of pre-made ready-to-use datasets.\n",
    "\n",
    "They provide libraries for both downloading models using a CLI or in your own script. \n",
    "\n",
    "To install the CLI simply run\n",
    "```\n",
    "pip install huggingface_hub\n",
    "```\n",
    "\n",
    "You will then need to provide a personal token to download models from Huggingface which you can do via `huggingface-cli login`.\n",
    "\n",
    "You can then download your model of choice with using the CLI with \n",
    "\n",
    "```\n",
    "huggingface-cli download <MODEL-ID>\n",
    "```\n",
    "Where the model-id is usually comprised of a group- and a model-name for example: `meta-llama/Llama-3.2-1B`\n",
    "\n",
    "This will by default download the model to the path specified by the `HF_HOME` environment variable. So if you want to change the folder, you will need to change this variable.\n",
    "\n",
    "Alternatively you can download the model within your python script using the huggingface python libraries which you download using:\n",
    "\n",
    "```\n",
    "pip install transformers accelerate datasets\n",
    "```\n",
    "\n",
    "To then download the model use the following code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Foundation Model\n",
    "# base_model_id   = \"meta-llama/Llama-3.2-1B\n",
    "# Instruct Model\n",
    "base_model_id   = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "base_model      = AutoModelForCausalLM.from_pretrained(base_model_id) \n",
    "# Each model comes with its own tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "print (f\"Successfully downloaded Model: {base_model.config._name_or_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `AutoModelForCausalLM` will automatically build the appropriate model for the specified ID and return an abstraction of it so you will never have to directly interface with the model. Each model also comes with it's own tokenizer. This will be installed alongside the model itself so you don't have to do anything extra to get it. Simply create an instance with the abstraction created by `AutoTokenizer` and youre good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inference**\n",
    "\n",
    "The abstractions of the transformers library make running inference on a model a trivial task. There are however some things that you will have to keep in mind in order to use them properly. \\\n",
    "The most important part is that the prompt gets transformed into the correct format. As discussed before instruction models expect a specific format that varies depending on which model youre using. Luckily the tokenizer for such models provides functionality for automatically applying the format the model requires if the input is provided in a commonly used structure. The most common structure, and the one we will be using here is in the form of a list of individual prompt object. Each of which contains a role and contant (aka. the message itself). This structure is widely accepted and can be transformed into whichever specific prompt fomat youre using automatically by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only necessary (and posssible) for instruct models\n",
    "prompt = [\n",
    "    # This is the role of the model\n",
    "    {\"role\": \"system\",  \"content\": \"You are a helpful AI Assistant.\"},\n",
    "    # This is the prompt the model will answer\n",
    "    {\"role\": \"user\",    \"content\": \"What is the meaning to life\"}\n",
    "    # We do not need an assistant entry, as it will be generated by the model\n",
    "]\n",
    "\n",
    "# Format prompt from the parquet format to the format that is expected by the\n",
    "# respective model \n",
    "# NOTE: foundation models do not have a chat template so this will fail\n",
    "formatted_prompt = tokenizer.apply_chat_template(prompt, tokenize=False)\n",
    "\n",
    "print (\"Formatted Prompt:\")\n",
    "print (formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the prompt has the required format the rest is pretty straight forward. Simply tokenize the prompt and move everything to your GPU. Then, call the generate method. There are a few parameters to tweak the behaviour which are explained in the example below, but it's not at all complicated. Finally simply decode the returned tokens and there you go: You've successfully run your own local LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For foundation models you can simply provide any text\n",
    "# formatted_prompt = \"The meaning of life is\"\n",
    "\n",
    "def run_inference(model, tokenizer, prompt):\n",
    "    # The tokenizer transforms the raw text into so called input ids, which we\n",
    "    # specify to be PyTorch tensors\n",
    "    tokenized_prompt    = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # The tokenizer returns two outputs, the tokenized input, the \"input ids\"\n",
    "    # and the \"attention mask\" which tells the model at which token to start\n",
    "    # generating\n",
    "    input_ids           = tokenized_prompt.input_ids\n",
    "    attention_mask      = tokenized_prompt.attention_mask\n",
    "    # Move the model as well as the inputs to your GPU\n",
    "    device          = \"cuda\"\n",
    "    model           = model.to(device) \n",
    "    input_ids       = input_ids.to(device)\n",
    "    attention_mask  = attention_mask.to(device)\n",
    "\n",
    "    # Auto regressively generate the output sequence(s). It will always return\n",
    "    # a list, even if you're just generating one output.\n",
    "    output_sequences = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask          = attention_mask,\n",
    "        # Defines how many tokens will be generated on top of the already \n",
    "        # existing prompt\n",
    "        max_new_tokens          = 512, \n",
    "        # Makes it so we don't always take the most likely next token, but \n",
    "        # sample from a distribution of likely next tokens. If this is disabled\n",
    "        # a given prompt will always return the same result.\n",
    "        do_sample               = True, \n",
    "        # Increases or decreases the probabilities of the less likely tokens\n",
    "        temperature             = 0.6,\n",
    "        # Consider only as many tokens as it takes to reach this cumulative \n",
    "        # probability\n",
    "        top_p                   = 0.9,\n",
    "        # How many tokens will at most be considered for the sampling\n",
    "        top_k                   = 10,\n",
    "        # How many sequences will be generated for the result\n",
    "        num_return_sequences    = 1,\n",
    "        pad_token_id            = tokenizer.eos_token_id,\n",
    "        eos_token_id            = tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Move the output sequences, inputs and model back into RAM\n",
    "    device              = \"cpu\"\n",
    "    output_sequences    = output_sequences.to(device)\n",
    "    model               = model.to(device) \n",
    "    input_ids           = input_ids.to(device)\n",
    "    attention_mask      = attention_mask.to(device)\n",
    "\n",
    "    # Remove the original prompt from the response to only get the generated \n",
    "    # answer\n",
    "    output_sequences = output_sequences[:, input_ids.shape[1]:]\n",
    "\n",
    "    # Decode the first answer from tokens back to plain text, ignore the \n",
    "    # special tokens to only get readable text\n",
    "    response = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "print (run_inference(base_model, tokenizer, formatted_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preparing the Data**\n",
    "\n",
    "In order to fine-tune your model, you of course need training data. This sectionn deals with exactly that. There's gonna be two separate parts. The first looks at how to prepare a pre-made dataset, the second shows you how to prepare your very own custom one. That part is a lot more complex and complicated so I would recommend for your first experiments you focus on the first one.\n",
    "\n",
    "Our first snippet here simply defines a helper function that we will use later to format the entire dataset. It's essentially just formatting and tokenizing the data, as we've already seen for the inference. The exact procedure depends on the structure of the used dataset, so you might have to adjust this to your specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format and tokenize the entire dataset. The exact steps \n",
    "# you will need to do here depend on the structure of your dataset.\n",
    "def tokenize_function(entry, tokenizer):\n",
    "    \n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        entry[\"messages\"], \n",
    "        tokenize = False\n",
    "    )     \n",
    "    \n",
    "    tokenized_text = tokenizer(\n",
    "        text            = formatted_text,\n",
    "        # Changing this value can significantly change VRAM usage, so always\n",
    "        # go with as little as possible\n",
    "        max_length      = 256,\n",
    "        padding         = \"max_length\",\n",
    "        truncation      = True,\n",
    "        return_tensors  = \"pt\"\n",
    "    )\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using a Pre-Existing Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Huggingface datasets library allows you to easily download every dataset available on there and seemlessly integrate it with the workflow weve prepared so far. \n",
    "\n",
    "Similar as to how `from_pretrained` automatically downloads the model specified with its ID, `load_dataset` does the same but for the dataset. One small nuance is that theres going to be 2 different splits of the dataset, one for training and one for evaluating. \\\n",
    "Once downloaded we simply need to format the data for our model. We can do this via the `map()` function which we simply pass the already introduced `tokenize_funciton()` to. `map()` further allows us to batch this progress, drastically increasing runtime performance. \n",
    "\n",
    "And that's already it, we've now done everything necessary to prepare the data for our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for a pre-made dataset directly from huggingface\n",
    "from datasets import load_dataset\n",
    "\n",
    "# This is the id of the main dataset\n",
    "dataset_id      = \"HuggingFaceTB/smoltalk\"\n",
    "# If you have a subdataset, you can specify that as well\n",
    "dataset_name    = \"numina-cot-100k\"\n",
    "\n",
    "training_data = load_dataset(\n",
    "    dataset_id, \n",
    "    dataset_name, \n",
    "    split=\"train\")\n",
    "\n",
    "validation_data = load_dataset(\n",
    "    dataset_id, \n",
    "    dataset_name, \n",
    "    split=\"test\")\n",
    "\n",
    "# You will need to tokenize (and in the case of instruction models format) \n",
    "# the dataset before you can use it to train your adapter\n",
    "train_data  = training_data.map(\n",
    "    tokenize_function, \n",
    "    # This batches the data, increasing the performance of the mapping\n",
    "    batched     = True,\n",
    "    # This will be shown in the progress bar while the mapping is in progress\n",
    "    desc        = \"Formatting Training Data\",\n",
    "    # These arguments will be passed through to your mapping function\n",
    "    fn_kwargs   = {\n",
    "        \"tokenizer\" : tokenizer})\n",
    "        \n",
    "val_data    = validation_data.map(\n",
    "    tokenize_function, \n",
    "    batched     = True,\n",
    "    desc        = \"Formatting Validation Data\",\n",
    "    fn_kwargs   = {\n",
    "        \"tokenizer\" : tokenizer})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating your own Dataset**\n",
    "\n",
    "Creating your own dataset is a lot more complex than taking a pre-made one. Of course as this is a very individual task, this is merely a representation of one possible workflow to create your own dataset. However it demonstrates some common techniques you can use and adapt to your specific use-case. \n",
    "\n",
    "The overarching method we will be using is to leverage an LLM to generate training examples for us, going off of some general structure/ prompt/ idea/ etc. we provide it. This saves a lot of time over manually writing a lot of exmples. It may take some consideration and analysis beforehand though to make sure the generated data is actually useful though. \n",
    "\n",
    "The snippet below is essentially just providing a logical structure that allows us to group useful information together from one specific PDF file. As mentioned this simply serves to illustrate that you will have to do some preparation of your data for optimal results. There's no silver bullet for this. However there are some libraries like `augmentoolkit`, or `distilabel` that may work for you if youre looking for a very general workflow. In most real cases you will however have to do some sort of your own preparation, especially if you're working with languages that aren't english or intend to use a local LLM for the generating part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bunch of identifying keywords that represent whenever a new batch of\n",
    "# information follows that is relevant for the fine-tuning dataset\n",
    "table_section_keywords = [\n",
    "    \"Studiengang\", \n",
    "    \"Kürzel\", \n",
    "    \"Bezeichnung\", \n",
    "    \"Lehrveranstaltung(en)\",\n",
    "    \"Verantwortliche(r)\",\n",
    "    \"Zuordnung zum Curriculum\",\n",
    "    \"Verwendbarkeit\",\n",
    "    \"Semesterwochenstunden\",\n",
    "    \"ECTS\",\n",
    "    \"Voraussetzungen\",\n",
    "    \"Dauer\",\n",
    "    # Submodule\n",
    "    \"Lehrveranstaltung\",\n",
    "    \"Dozent(en)\",\n",
    "    \"Hörtermin\",\n",
    "    \"Häufigkeit\",\n",
    "    \"Art\",\n",
    "    \"Lehrform\",\n",
    "    \"Semesterwochenstunden\",\n",
    "    \"ECTS\",\n",
    "    \"Prüfungsform\",\n",
    "    \"Sprache\",\n",
    "    \"Lehr- und Medienform(en)\",\n",
    "]\n",
    "\n",
    "content_keywords = [\n",
    "    \"Lernziele\",\n",
    "    \"Inhalt\",\n",
    "    \"Literatur\",\n",
    "]\n",
    "\n",
    "# Helper function to determine whether a line of starts with a defined keyword\n",
    "def starts_with_any(string, keywords):\n",
    "    for k in keywords:\n",
    "        if string.startswith(k):\n",
    "            return k\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "# Helper class that contains a pages content. This is highly individual and\n",
    "# will depend greatly on whatever source data you are working with.\n",
    "#\n",
    "# The exact workings of this class is really not relevant, it's simply a \n",
    "# representative of a page divided into logical chunks that contain information\n",
    "# of some sort that we want to feed into the LLM\n",
    "class Page():\n",
    "    def __init__(self, raw_text):\n",
    "        self.raw_text           = raw_text\n",
    "\n",
    "        self.name               = \"\"\n",
    "        self.is_submodule       = False\n",
    "\n",
    "        self.table_paragraphs   = []\n",
    "        self.content_paragraphs = []\n",
    "\n",
    "        latest_access           = None\n",
    "\n",
    "        for line in raw_text.splitlines():\n",
    "            parsed_line = \"\"\n",
    "\n",
    "            # New logical Page\n",
    "            if line.startswith (\"I.\"):\n",
    "                self.name = \" \".join(line.split(\" \")[1:])\n",
    "                self.is_submodule = line.split()[0].count(\".\") >= 3\n",
    "                continue\n",
    "            \n",
    "            # Table section\n",
    "            keyword = starts_with_any(line, table_section_keywords)\n",
    "            if keyword:\n",
    "                parsed_line = keyword + \":\" + line[len(keyword):]\n",
    "                self.table_paragraphs.append(parsed_line)\n",
    "                latest_access = self.table_paragraphs\n",
    "                continue\n",
    "            \n",
    "            # Content section\n",
    "            keyword = starts_with_any(line, content_keywords)\n",
    "            if keyword:\n",
    "                parsed_line = keyword + \":\" + line[len(keyword):]\n",
    "                self.content_paragraphs.append(parsed_line)\n",
    "                latest_access = self.content_paragraphs\n",
    "                continue\n",
    "            \n",
    "            # Whether this logical page is concluded or not\n",
    "            if latest_access:\n",
    "                latest_access[-1] += \"\\n\" + line\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return (self.name \n",
    "                + \"\\n\" \n",
    "                + str(self.table_paragraphs) \n",
    "                + \"\\n\" \n",
    "                + str(self.content_paragraphs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just some prep for the intermediate results we will get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_path       = \"custom_data\"\n",
    "source_path     = os.path.join(save_path, \"source.pdf\")\n",
    "raw_data_path   = os.path.join(save_path, \"raw_data.txt\")\n",
    "data_path       = os.path.join(save_path, \"data.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet now does all the heavy lifting for the generating part. First we simply read our source data and convert it into our logical information representation. Then follows the important part. Using the logical chunks of information we generate a prompt for our local LLM that details the result we want, the dataformat it should have and we also provide an example for a question and corresponding answer. This process should ideally use a fairly powerful LLM as the quality of this data matter immensely. It should be noted that it does not need to be the same as the model you're looking to fine-tune. \n",
    "\n",
    "Generating the question and answer pairs may take quite a while so in this snippet we've just taken a tiny slice of the actual prepared data for demonstration purposes. The interim results will be saved in the `raw_data.txt` so you can have a look at what such a result may look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 83\u001b[0m\n\u001b[0;32m     78\u001b[0m responses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m tqdm(prepared_pages[\u001b[38;5;241m6\u001b[39m:\u001b[38;5;241m7\u001b[39m]):\n\u001b[0;32m     80\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     81\u001b[0m         generate_question_pairs(\n\u001b[0;32m     82\u001b[0m             page, \n\u001b[1;32m---> 83\u001b[0m             \u001b[43mbase_model\u001b[49m,\n\u001b[0;32m     84\u001b[0m             tokenizer)\n\u001b[0;32m     85\u001b[0m     )\n\u001b[0;32m     87\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(raw_data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "import  json\n",
    "from    pypdf  import PdfReader\n",
    "from    tqdm   import tqdm\n",
    "\n",
    "# Load our source Data and prepare it to a list of chapters\n",
    "reader = PdfReader(source_path)\n",
    "\n",
    "parsed_pdf          = []\n",
    "finished_preamble   = False \n",
    "\n",
    "# Again the exact workings are irrelevant as its specific to the data you are\n",
    "# working with\n",
    "for page in reader.pages:\n",
    "    page_text = page.extract_text()\n",
    "\n",
    "    if page_text.startswith(\"I.\"):\n",
    "        finished_preamble = True\n",
    "        parsed_pdf.append(page_text)\n",
    "    \n",
    "    elif finished_preamble:\n",
    "        parsed_pdf[-1] = parsed_pdf[-1] + \"\\n\" + page_text\n",
    "\n",
    "    elif len(page_text):\n",
    "        parsed_pdf.append(page_text)\n",
    "\n",
    "# After we've created the list of chapter texts, we can convert them to our\n",
    "# previously established logical representation, in this case the Page() class\n",
    "prepared_pages = []\n",
    "for page in parsed_pdf:\n",
    "    prepared_pages.append(Page(page))\n",
    "\n",
    "with open(\"prompts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "def build_prompt (page, content, amount):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": prompts[\"sys_qa_prompt\"]},\n",
    "        {\"role\": \"user\", \"content\": \n",
    "            prompts[\"user_course_intro\"] \n",
    "            + page.name\n",
    "            + prompts[\"user_question_amount_pre\"] \n",
    "            + str(amount)\n",
    "            + prompts[\"user_question_amount_post\"]\n",
    "            + prompts[\"user_course_name_condition\"]\n",
    "            + page.name\n",
    "            + prompts[\"user_one_shot_example\"]\n",
    "            + prompts[\"user_page_content_intro\"]\n",
    "            + content\n",
    "        }   \n",
    "    ]\n",
    "\n",
    "\n",
    "# Once we've prepared the pages we can then generate prompts for our LLM to \n",
    "# answer. For this we first define a function that generates our prompts from\n",
    "# the previously created logical chunks of information\n",
    "def generate_question_pairs(page, \n",
    "                            model,\n",
    "                            tokenizer):\n",
    "\n",
    "    question_answer_pairs = []\n",
    "    for table_content in page.table_paragraphs:\n",
    "        prompt      = build_prompt(page, table_content, 4)\n",
    "        prompt      = tokenizer.apply_chat_template(prompt, tokenize = False)\n",
    "        response    = run_inference(model, tokenizer, prompt)\n",
    "        \n",
    "        question_answer_pairs.append(response)\n",
    "\n",
    "    for table_content in page.content_paragraphs:\n",
    "        prompt      = build_prompt(page, table_content, 10)\n",
    "        prompt      = tokenizer.apply_chat_template(prompt, tokenize = False)\n",
    "        response    = run_inference(model, tokenizer, prompt)\n",
    "\n",
    "        question_answer_pairs.append(response)\n",
    "    \n",
    "    return question_answer_pairs\n",
    "\n",
    "# Short example for a test run. a normal run would take a lot longer than this!\n",
    "responses = []\n",
    "for page in tqdm(prepared_pages[6:7]):\n",
    "    responses.append(\n",
    "        generate_question_pairs(\n",
    "            page, \n",
    "            base_model,\n",
    "            tokenizer)\n",
    "    )\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "with open(raw_data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for response in responses:\n",
    "        for r in response: \n",
    "            f.write(r + \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that was generated this way will still have some artifacts of your LLMs answers such as \"Sure, here are X results\", etc. There may also be incorrectly formatted results, incomplete ones, you get the idea. The quality of your source data, prompt and LLM all play a part in this so you may have to adjust one if not all of those components, if the quality appears to be below expectations. \n",
    "\n",
    "To fix some of the formatting issues theres a simple script below that filters out most unusable lines. There may however still be the need for some minor manual adjustments afterwards. This final prepared data can be found in the `data.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we have prepared a raw data document we still need to properly format\n",
    "# and clean it in such a way that no remnants or artifacts remain from the LLMs\n",
    "# answers\n",
    "\n",
    "with open(raw_data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read() \n",
    "\n",
    "# Ensure that each line represents a valid JSON object\n",
    "formatted_data = []\n",
    "for line in content.splitlines():\n",
    "    cleaned_line = line.lstrip().rstrip()\n",
    "    \n",
    "    # Just some simple checks to see if the formatting is correct.\n",
    "    if not cleaned_line.startswith(\"{\") or \"}\" not in cleaned_line:\n",
    "        continue\n",
    "\n",
    "    if not cleaned_line.endswith(\",\"):\n",
    "        if not cleaned_line.endswith(\"}\"):\n",
    "            continue\n",
    "        cleaned_line += \",\"\n",
    "\n",
    "    formatted_data.append(cleaned_line)\n",
    "\n",
    "# Save the objects surrounded by a list to make it a valid JSON file\n",
    "with open(data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"[\\n\")\n",
    "    for line in formatted_data:\n",
    "        f.write(line + \"\\n\") \n",
    "    f.write(\"]\")\n",
    "\n",
    "# NOTE: Depending on your models strength you may have to remove some artifacts\n",
    "# manually from the generated data.json as the auto formatting is fairly naive.\n",
    "# This should at most be a few lines though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've prepared our dataset, we still need to make it usable for the LLM we want to fine-tune. This is achieved in the snippet below. Essentially all it's doing is just loading our `data.json` transforming and transforming it into the universally accepted stucture we've seen before and subsequently formatting and tokenizing it as we've already done for the pre-made dataset earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# now to actually create the datasets from the prepared QA pairs\n",
    "def data_mapper(entry):\n",
    "    result_s = {}\n",
    "    result_s[\"role\"]    = \"system\"\n",
    "    result_s[\"content\"] = prompts[\"sys_training_prompt\"]\n",
    "\n",
    "    result_q = {}\n",
    "    result_q[\"role\"]    = \"user\"\n",
    "    result_q[\"content\"] = entry[\"Q\"]\n",
    "\n",
    "    result_a = {}\n",
    "    result_a[\"role\"]    = \"assistant\"\n",
    "    result_a[\"content\"] = entry[\"A\"]\n",
    "\n",
    "    return [result_s, result_q, result_a]\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "training_data   = data[:50]\n",
    "validation_data = data[50:60]\n",
    "\n",
    "training_data   = {\n",
    "    \"messages\": [data_mapper(entry) for entry in training_data]}\n",
    "validation_data = {\n",
    "    \"messages\": [data_mapper(entry) for entry in validation_data]}\n",
    "\n",
    "training_data   = Dataset.from_dict(training_data)\n",
    "validation_data = Dataset.from_dict(validation_data)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "training_data = training_data.map(\n",
    "    tokenize_function, \n",
    "    batched     = True,\n",
    "    desc        = \"Formatting Training Data\",\n",
    "    fn_kwargs   = {\n",
    "        \"tokenizer\" : tokenizer})\n",
    "\n",
    "\n",
    "validation_data = validation_data.map(\n",
    "    tokenize_function, \n",
    "    batched     = True,\n",
    "    desc        = \"Formatting Validation Data\",\n",
    "    fn_kwargs   = {\n",
    "        \"tokenizer\" : tokenizer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it. Your completely original dataset made from scratch and entirely usable from now on for the fine-tuning process. Again its important to mention that all of this is just exemplary of one possible, albeit common, simple workflow of preparing fine-tuning data. You will have to make many adjustments and refinements to make this viable for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fine-Tuning the Base-Model**\n",
    "\n",
    "Finally after all these steps we can get to actually fine-tuning the base model using the previously prepared dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Selecting a Framework**\n",
    "\n",
    "As you'll likely notice the process for unsloth and PEFT is remarkably similar. As unsloth provides some excellent runtime and memory optimizations, while being extremely simple to use, I recommend you use that if you have the choice. The one main downside of it however is that it only functions on Linux machines. So if you don't have that option you'll have to go with one of the other options. \n",
    "\n",
    "The other two options we will be examining here are huggingfaces PEFT and torchtune. Torchtune is provided directly by Meta and is a native extension of PyTorch. Because of this it has some nice runtime and memory benefits, the one main drawback being that it supports a rather limited amount of models. However they do support the Llama model family which provide excellent performance for open source also supporting a wide variety of model sizes. A little bit of a con is that it can be a bit tricky to include it into your own workflow as it is mainly designed to be used as a command line tool. It does provide a variety of standalone scripts however that you can copy and adapt to fit into your own architecture. The needed config and adapted script are provided in the `torchtune_custom` folder. The adapted excerpts have been marked as such if you want to have a look at it. For more scripts and configs that may cater to different use cases such as full fine-tuning, different models or multi-GPU clusters you can check our their [GitHub repository](https://github.com/pytorch/torchtune).\n",
    "\n",
    "If torchtune isn't an option either, then you can always go with PEFT which basically supports every model and system you throw at it. It may be less efficient depending on your use-case but it's still a great fallback catch-all solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently supported: HF PEFT | torchtune | unsloth (only linux)\n",
    "fine_tuning_framework = \"PEFT\"\n",
    "\n",
    "match fine_tuning_framework:\n",
    "    # Using Hugging Faces PEFT implementation =================================\n",
    "    case \"PEFT\":\n",
    "        from transformers   import AutoModelForCausalLM, AutoTokenizer\n",
    "        from peft           import get_peft_model, LoraConfig\n",
    "\n",
    "        base_model  = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
    "        tokenizer   = AutoTokenizer.from_pretrained(base_model_id)\n",
    "        \n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        lora_config = LoraConfig (\n",
    "            # LoRA rank, higher value = better performance but more ressources\n",
    "            r               = 16,\n",
    "            # Weight of the adapter, usually 2x the rank\n",
    "            lora_alpha      = 32, \n",
    "            lora_dropout    = 0,  \n",
    "            bias            = \"none\",  \n",
    "            task_type       = \"CAUSAL_LM\",\n",
    "            # Matrices that we want to adapt  \n",
    "            target_modules=[\"q_proj\", \n",
    "                            \"v_proj\", \n",
    "                            \"k_proj\", \n",
    "                            \"o_proj\",\n",
    "                            \"gate_proj\", \n",
    "                            \"down_proj\", \n",
    "                            \"up_proj\"],\n",
    "        )\n",
    "        # This is the frozen base model + the adapter which we want to train\n",
    "        adapted_model = get_peft_model(base_model, lora_config)\n",
    "        \n",
    "        print (\"Fine-Tuning with PEFT!\")\n",
    "    \n",
    "    # Using unsloth ===========================================================\n",
    "    case \"unsloth\":\n",
    "        import platform\n",
    "        # Merely importing the framework wont work if youre not on linux so\n",
    "        # we have this early out\n",
    "        if platform.system() != \"Linux\":\n",
    "            logging.error(\"Unsloth is only supported on Linux!\")\n",
    "            exit()\n",
    "        from unsloth    import FastLanguageModel\n",
    "        \n",
    "        base_model, tokenizer   = FastLanguageModel.from_pretrained(\n",
    "            model_name      = base_model_id,\n",
    "            max_seq_length  = 1024,\n",
    "            # Not all GPUs support this\n",
    "            dtype           = \"bf16\",\n",
    "            # Only for quantized models\n",
    "            load_in_4bit    = False, \n",
    "        )\n",
    "        # As you can see it a very similar process to the PEFT one\n",
    "        adapted_model = FastLanguageModel.get_peft_model(\n",
    "            base_model,\n",
    "            r               = 16,\n",
    "            lora_alpha      = 32,\n",
    "            lora_dropout    = 0,\n",
    "            bias            = \"none\",\n",
    "        )\n",
    "\n",
    "        print (\"Fine-Tuning with unsloth!\")\n",
    "\n",
    "    # Using torchtune =========================================================\n",
    "    case \"torchtune\":\n",
    "        import yaml\n",
    "        # The format torchtune expects\n",
    "        from omegaconf                      import OmegaConf\n",
    "        # Customized recipe, be sure to check out the customized sections\n",
    "        # which make it possible to integrate it into our system\n",
    "        from torchtune_custom.fine_tuning   import LoRAFinetuneRecipeSingleDevice\n",
    "        from transformers                   import AutoTokenizer\n",
    "        \n",
    "        # Because we tokenize the dataset ourself we also instantiate it \n",
    "        # ourself\n",
    "        tokenizer           = AutoTokenizer.from_pretrained(base_model_id)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        with open(\"torchtune_custome/fine-tune.yaml\", \"r\") as file:\n",
    "            cfg = yaml.safe_load(file)\n",
    "\n",
    "        # The config handles the entire details for the training behaviour, so\n",
    "        # definitely have a look at it if youre choosing torchtune.        \n",
    "        cfg     = OmegaConf.create(cfg)\n",
    "        recipe  = LoRAFinetuneRecipeSingleDevice(cfg=cfg)\n",
    "\n",
    "        print (\"Fine-Tuning with Torchtune\")\n",
    "        \n",
    "    case _:\n",
    "        print (\"This framework is currently not supported\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once youve created the adapters and set up the configs, running the training procedure is very very easy for each option. Merely a couple of lines each. \n",
    "Torchtune does not natively support custom datasets, so there are some adjustments you will have to make in the script to allow you to use them. All of the respective changes have been marked, so it should be easy to tell where they are going.\n",
    "One thing of note though if you are using torchtune is that it expects a very specific formatting for the data. Its very similar to the one we are using right now it just expects the fields to have different names, so we have to rename them before being able to pass it in.\n",
    "\n",
    "The fine tuning process may or may not take a while depending on your dataset size. If you havent changed anything so far we are using a very little slice of the data, so it shouldn't take long at all, but the results will also not be of great quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fine_tuning_framework' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m fine_tuned_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m \u001b[43mfine_tuning_framework\u001b[49m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Unsloth and PEFT use the exact same training routine!>\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPEFT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m   \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (DataCollatorForLanguageModeling, \n\u001b[0;32m      7\u001b[0m                                     TrainingArguments)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fine_tuning_framework' is not defined"
     ]
    }
   ],
   "source": [
    "fine_tuned_dir = \"result\"\n",
    "\n",
    "match fine_tuning_framework:\n",
    "    # Unsloth and PEFT use the exact same training routine!>\n",
    "    case \"PEFT\" | \"unsloth\":\n",
    "        from transformers   import (DataCollatorForLanguageModeling, \n",
    "                                    TrainingArguments)\n",
    "        from trl            import SFTTrainer\n",
    "        \n",
    "        # This collects the data for the training\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer, \n",
    "            # We dont want masked language modelling, instead we just want to\n",
    "            # predict the next token\n",
    "            mlm = False\n",
    "        )\n",
    "        # These are just the usual training arguments you would expect for a \n",
    "        # trainer\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir                  = fine_tuned_dir,\n",
    "            num_train_epochs            = 1,\n",
    "            per_device_train_batch_size = 1,\n",
    "            per_device_eval_batch_size  = 1,\n",
    "            eval_strategy               = \"epoch\",\n",
    "            logging_dir                 = \"./logs\",\n",
    "            logging_steps               = 500,\n",
    "            save_steps                  = 500,\n",
    "            save_total_limit            = 2,\n",
    "            bf16                        = True,  \n",
    "        )\n",
    "        # SFTTrainer is specifically for fine-tuning. (Supervised Fine-Tuning\n",
    "        # Trainer)\n",
    "        trainer = SFTTrainer(\n",
    "            model           = adapted_model, \n",
    "            train_dataset   = training_data,\n",
    "            eval_dataset    = validation_data,\n",
    "            data_collator   = data_collator,\n",
    "            args            = training_args,\n",
    "        )\n",
    "\n",
    "        # Depending on your machine this may take quite a while\n",
    "        trainer.train()\n",
    "        \n",
    "        # This will only save the adapter weights\n",
    "        adapted_model.save_pretrained(fine_tuned_dir)\n",
    "        print (f\"Saved adapter at {str(fine_tuned_dir)}\")\n",
    "\n",
    "    # Using torchtune =========================================================\n",
    "    case \"torchtune\":\n",
    "        from tqdm import tqdm\n",
    "        # Torchtune expects a specific format for the data, so we need to\n",
    "        # format it first\n",
    "        training_data = [\n",
    "            {\n",
    "                \"tokens\":   data[\"input_ids\"], \n",
    "                \"mask\":     data[\"attention_mask\"], \n",
    "                \"labels\":   data[\"input_ids\"]\n",
    "\n",
    "            } for data in tqdm(train_data)\n",
    "        ]\n",
    "\n",
    "        # All of the actual configuration is done in the fine-tune.yaml\n",
    "        recipe.setup (\n",
    "            cfg        = cfg, \n",
    "            # The recipe has some custom adjustments to allow us to pass in a \n",
    "            # tokenizer and custom dataset. This would not be possible natively\n",
    "            # so youll have to take a look at that beforehand if youre planning\n",
    "            # to use custom data. \n",
    "            dataset    = train_data,\n",
    "            tokenizer  = tokenizer\n",
    "        )\n",
    "\n",
    "        recipe.train()\n",
    "        recipe.cleanup()\n",
    "\n",
    "    case _:\n",
    "        print (\"This framework is currently not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can find the adapter weights in the `result` directory. As you can see a whole bunch of files have been generated, most of which are just to provide additional information and configuration for the adapter. The actual weights are stored in the `.safetensors` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading your Fine-Tuned Model Adapter**\n",
    "\n",
    "Now that you've created your LoRA adapter running inference with it is extremely easy and very similar to how youve done it before. All you really need to do is load the additional adapter youve trained and combine it with the base-model that you'll load exactly the same as you have before.  After that it can be used exactly as the base model, and you can observe it's new behaviour right there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the fine tuned model is almost the same for each method!\n",
    "from peft import PeftModel\n",
    "\n",
    "# Because we only save the adapted weights we still need to load the base model\n",
    "# first\n",
    "base_model          = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
    "tokenizer           = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# The base model gets overriden in this case so dont use it after!\n",
    "fine_tuned_model    = PeftModel.from_pretrained(base_model, fine_tuned_dir)\n",
    "base_model          = None\n",
    "\n",
    "if fine_tuning_framework == \"unsloth\":\n",
    "    fine_tuned_model = FastLanguageModel.for_inference(fine_tuned_model)\n",
    "\n",
    "# You can now use the fine tuned_model as if you were using the normal base \n",
    "# model. Note that the tokenizer does not change!\n",
    "prompt = [\n",
    "    {\"role\": \"system\",  \"content\": \"Du bist ein KI Assistent der FH Wedel.\"},\n",
    "    {\"role\": \"user\",    \"content\": \"Was ist die Modulnummer von Algorithmics.\"}\n",
    "]\n",
    "formatted_prompt    = tokenizer.apply_chat_template(prompt, tokenize=False)\n",
    "\n",
    "print (run_inference(fine_tuned_model, tokenizer, formatted_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **You're Done!** \n",
    "\n",
    "Don't forget to restart the notebook to free up the ressources used by the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LoRA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
